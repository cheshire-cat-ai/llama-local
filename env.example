##############################
# Cheshire Cat Env Variables #
##############################

# Decide host and port for your Cat. Default will be localhost:1865
CORE_HOST=localhost
CORE_PORT=1865

# Qdrant server
# QDRANT_HOST=localhost
# QDRANT_PORT=6333

# Decide to use https / wss secure protocols
#CORE_USE_SECURE_PROTOCOLS=true

# Protect endpoints with an access token
#API_KEY=meow

# Log levels
LOG_LEVEL=WARNING

# Turn on memory collections' snapshots on embedder change with SAVE_MEMORY_SNAPSHOTS=true
SAVE_MEMORY_SNAPSHOTS=false

##################################
# Llama-cpp server Env Variables #
##################################

# Ip address and port from which to accept incoming connections
# LLAMA_HOST=0.0.0.0
# LLAMA_PORT=8000

# Folder containing the models. This is were Docker mounts a volume
# Mandatory!
# MODELS_FOLDER=models

# Name of the model file. This should be inside MODELS_FOLDER
# Mandatory!
# MODEL=...

# Length in tokens of the model's context
# N_CTX=2048

# Number of model's layer to off-load on the GPU
# N_GPU_LAYERS=0

# Index of the main GPU
# MAIN_GPU=0

# Number of CPU threads
# N_THREADS=4

# Number of token to consider for repetition penalty
# LAST_N_TOKENS_SIZE=64

# Format of the conversation to use
# CHAT_FORMAT=llama-2

# Whether to interrupt requests when a new request is received
# INTERRUPT_REQUESTS=True

# Verbose=True
